---
clusterName: "elasticsearch"
nodeGroup: "master"
nameOverride: "elasticsearch"

# The service that non master groups will try to connect to when joining the cluster
# This should be set to clusterName + "-" + nodeGroup for your master group
masterService: "elasticsearch-master"

# Elasticsearch roles that will be applied to this nodeGroup
# These will be set as environment variables. E.g. node.roles=master
# https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles
roles:
  - master
  - data
  - data_content
  - data_hot
  - data_warm
  - data_cold
  - ingest
  - ml
  - remote_cluster_client
  - transform

replicas: 1
minimumMasterNodes: 1


createCert: true


image: "docker.elastic.co/elasticsearch/elasticsearch"
imageTag: "8.1.0"
imagePullPolicy: "IfNotPresent"

extraEnvs:
        #  - name: ELASTIC_PASSWORD
        #    valueFrom:
        #      secretKeyRef:
        #        name: elasticsearach-master-credentials
        #        key: password
        #  - name: ELASTIC_USERNAME
        #    valueFrom:
        #      secretKeyRef:
        #        name: elasticsearach-master-credentials
        #        key: username
        #
  - name: KIBANA_PASSWORD
    valueFrom:
      secretKeyRef:
        name: kibana-credentials
        key: password
  - name: KIBANA_USERNAME
    valueFrom:
      secretKeyRef:
        name: kibana-credentials
        key: username
        #  - name: httpProxy
        #    value: "http://185.174.248.254:4912"
        #  - name: httpsProxy
        #    value: "http://185.174.248.254:4912"
        #  - name: noProxy
        #    value: "svc,localhost,local,127.0.0.1"
        #

secret:
  enabled: true
  password: "ZhbmBk1sTDmjEmq6"

esJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "1000m"
    memory: "2Gi"
  limits:
    cpu: "1000m"
    memory: "2Gi"


networkHost: "0.0.0.0"

#volumeClaimTemplate:
#  storageClassName: local
#  accessModes: ["ReadWriteOnce"]
#  resources:
#    requests:
#      storage: 5Gi
#

podSecurityPolicy:
  create: false
  name: "elk-policy"
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim
      - emptyDir

persistence:
  enabled: false
  labels:
    # Add default labels for the volumeClaimTemplate of the StatefulSet
    enabled: false
    #  annotations: {}
    #
    #extraVolumes: 
    # - name: elk-data
    #   hostPath:
    #     path: /var/opt/elasticsearch
    #     type: DirectoryOrCreate
    #
    #
    #extraVolumeMounts:
    #  - name: elk-data
    #    mountPath: /usr/share/elasticsearch/data
    #

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: "kubernetes.io/hostname"

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort"
antiAffinity: "hard"


# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

# The environment variables injected by service links are not used, but can lead to slow Elasticsearch boot times when
# there are many services in the current namespace.
# If you experience slow pod startups you probably want to set this to `false`.
enableServiceLinks: true

protocol: https
httpPort: 9200
transportPort: 9300

service:
  enabled: true
  type: ClusterIP
  publishNotReadyAddresses: true
  httpPortName: http
  transportPortName: transport
service:
  enabled: true
  type: NodePort
  publishNotReadyAddresses: true
  httpPortName: http
  transportPortName: transport


updateStrategy: RollingUpdate

# This is the max unavailable setting for the pod disruption budget
# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 1

podSecurityContext:
  fsGroup: 1000
  runAsUser: 1000

securityContext:
  capabilities:
    drop:
      - ALL
  # readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

# How long to wait for elasticsearch to stop gracefully
terminationGracePeriod: 120

sysctlVmMaxMapCount: 262144

readinessProbe:
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 3
  timeoutSeconds: 5

# https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##

# Enabling this will publicly expose your Elasticsearch instance.
# Only enable this if you have security enabled on your cluster
ingress:
  enabled: false
#  kubernetes.io/ingress.class: nginx
#  kubernetes.io/tls-acme: "true"
#  className: "nginx"
#  pathtype: ImplementationSpecific
#  hosts:
#    - host: chart-example.local
#      paths:
#        - path: /
#

sysctlInitContainer:
  enabled: true


  ##networkPolicy:
  ## Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.
  ## In order for a Pod to access Elasticsearch, it needs to have the following label:
  ## {{ template "uname" . }}-client: "true"
  ## Example for default configuration to access HTTP port:
  ##  elasticsearch-master-http-client: "true"
  ## Example for default configuration to access transport port:
  ##  elasticsearch-master-transport-client: "true"

  #  http:
  #    enabled: true
    ## if explicitNamespacesSelector is not set or set to {}, only client Pods being in the networkPolicy's namespace
    ## and matching all criteria can reach the DB.
    ## But sometimes, we want the Pods to be accessible to clients from other namespaces, in this case, we can use this
    ## parameter to select these namespaces
    ##
    # explicitNamespacesSelector:
    #   # Accept from namespaces with all those different rules (only from whitelisted Pods)
    #   matchLabels:
    #     role: frontend
    #   matchExpressions:
    #     - {key: role, operator: In, values: [frontend]}

    ## Additional NetworkPolicy Ingress "from" rules to set. Note that all rules are OR-ed.
    ##
    # additionalRules:
    #   - podSelector:
    #       matchLabels:
    #         role: frontend
    #   - podSelector:
    #       matchExpressions:
    #         - key: role
    #           operator: In
    #           values:
    #             - frontend

  #  transport:
    ## Note that all Elasticsearch Pods can talk to themselves using transport port even if enabled.
    #    enabled: true
    # explicitNamespacesSelector:
    #   matchLabels:
    #     role: frontend
    #   matchExpressions:
    #     - {key: role, operator: In, values: [frontend]}
    # additionalRules:
    #   - podSelector:
    #       matchLabels:
    #         role: frontend
    #   - podSelector:
    #       matchExpressions:
    #         - key: role
    #           operator: In
    #           values:
    #             - frontend

# Deprecated
# please use the above podSecurityContext.fsGroup instead
